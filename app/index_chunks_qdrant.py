#!/usr/bin/env python3
"""
Qdrant ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

data/chunks.jsonl ã‚’èª­ã¿è¾¼ã¿ã€Qdrant ã«ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
"""

import json
import os
import time
import uuid
from pathlib import Path
from typing import Dict, List, Any

from dotenv import load_dotenv
from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# .env ã‚’ api-server ç›´ä¸‹ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
env_path = Path(__file__).resolve().parents[1] / ".env"
load_dotenv(env_path)

# è¨­å®š
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
COLLECTION_NAME = "neko_scenes"
EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-small")
BATCH_SIZE = 100

# ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
CHUNKS_FILE = Path(__file__).resolve().parents[1] / "data" / "chunks.jsonl"


def get_embedding_dimension(client: OpenAI, model: str) -> int:
    """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°ã‚’å–å¾—"""
    print(f"ğŸ“ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« '{model}' ã®æ¬¡å…ƒæ•°ã‚’å–å¾—ä¸­...")
    response = client.embeddings.create(
        model=model,
        input="test"
    )
    dimension = len(response.data[0].embedding)
    print(f"âœ“ æ¬¡å…ƒæ•°: {dimension}")
    return dimension


def create_collection(qdrant: QdrantClient, collection_name: str, dimension: int):
    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆï¼ˆæ—¢å­˜ãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤ã—ã¦å†ä½œæˆï¼‰"""
    print(f"\nğŸ—‚ï¸  ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã‚’æº–å‚™ä¸­...")

    # æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ç¢ºèª
    collections = qdrant.get_collections().collections
    exists = any(col.name == collection_name for col in collections)

    if exists:
        print(f"âš ï¸  æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã‚’å‰Šé™¤ä¸­...")
        qdrant.delete_collection(collection_name)
        print("âœ“ å‰Šé™¤å®Œäº†")

    # æ–°è¦ä½œæˆ
    print(f"âœ“ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã‚’ä½œæˆä¸­...")
    qdrant.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=dimension, distance=Distance.COSINE)
    )
    print("âœ“ ä½œæˆå®Œäº†")


def load_chunks(filepath: Path) -> List[Dict[str, Any]]:
    """chunks.jsonl ã‚’èª­ã¿è¾¼ã¿"""
    print(f"\nğŸ“– {filepath} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    chunks = []
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            chunks.append(json.loads(line.strip()))
    print(f"âœ“ {len(chunks)} ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã¿")
    return chunks


def embed_texts(client: OpenAI, texts: List[str], model: str) -> List[List[float]]:
    """ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
    response = client.embeddings.create(
        model=model,
        input=texts
    )
    return [data.embedding for data in response.data]


def index_chunks_to_qdrant(
    qdrant: QdrantClient,
    openai_client: OpenAI,
    chunks: List[Dict[str, Any]],
    collection_name: str,
    embed_model: str,
    batch_size: int
):
    """ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒãƒƒãƒå‡¦ç†ã§ Qdrant ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
    print(f"\nğŸš€ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆé–‹å§‹ (ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size})")

    total = len(chunks)
    indexed_count = 0

    for i in range(0, total, batch_size):
        batch = chunks[i:i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (total + batch_size - 1) // batch_size

        print(f"ğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches} (ä»¶æ•°: {len(batch)}) ã‚’å‡¦ç†ä¸­...")

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿
        texts = [chunk["text"] for chunk in batch]
        embeddings = embed_texts(openai_client, texts, embed_model)

        # ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆ
        points = []
        for chunk, embedding in zip(batch, embeddings):
            point_id = str(uuid.uuid4())
            payload = {
                "scene_id": chunk["id"],
                "scene_index": chunk["scene_index"],
                "chapter": chunk["chapter"],
                "start_pos": chunk["start_pos"],
                "end_pos": chunk["end_pos"],
                "characters": chunk["characters"],
                "text": chunk["text"]
            }
            points.append(PointStruct(
                id=point_id,
                vector=embedding,
                payload=payload
            ))

        # Qdrant ã«ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ
        qdrant.upsert(
            collection_name=collection_name,
            points=points
        )

        indexed_count += len(batch)
        print(f"âœ“ {indexed_count}/{total} ä»¶å®Œäº†")

        # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚å°‘ã—å¾…æ©Ÿ
        if i + batch_size < total:
            time.sleep(0.5)

    print(f"\nâœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†: {indexed_count} ä»¶")


def print_statistics(chunks: List[Dict[str, Any]]):
    """çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›"""
    print("\nğŸ“Š çµ±è¨ˆæƒ…å ±:")
    print(f"  ç·ä»¶æ•°: {len(chunks)}")

    if chunks:
        first = chunks[0]
        last = chunks[-1]
        print(f"  æœ€åˆã® scene_index: {first['scene_index']} (ç«  {first['chapter']})")
        print(f"  æœ€å¾Œã® scene_index: {last['scene_index']} (ç«  {last['chapter']})")

        # ç« ã”ã¨ã®ä»¶æ•°
        chapters = {}
        for chunk in chunks:
            ch = chunk["chapter"]
            chapters[ch] = chapters.get(ch, 0) + 1

        print(f"  ç« ã®æ•°: {len(chapters)}")
        print(f"  ç« ã”ã¨ã®ä»¶æ•°: {dict(sorted(chapters.items()))}")


def main():
    print("=" * 60)
    print("ğŸ± å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ - Qdrant ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ")
    print("=" * 60)

    # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    # Qdrant ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    print(f"\nğŸ”Œ Qdrant ã«æ¥ç¶šä¸­ ({QDRANT_HOST}:{QDRANT_PORT})...")
    qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
    print("âœ“ æ¥ç¶šæˆåŠŸ")

    # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã‚’å–å¾—
    dimension = get_embedding_dimension(openai_client, EMBED_MODEL)

    # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
    create_collection(qdrant, COLLECTION_NAME, dimension)

    # ãƒãƒ£ãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã¿
    chunks = load_chunks(CHUNKS_FILE)

    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    print_statistics(chunks)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    index_chunks_to_qdrant(
        qdrant=qdrant,
        openai_client=openai_client,
        chunks=chunks,
        collection_name=COLLECTION_NAME,
        embed_model=EMBED_MODEL,
        batch_size=BATCH_SIZE
    )

    # æœ€çµ‚ç¢ºèª
    print("\nğŸ” ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±:")
    collection_info = qdrant.get_collection(COLLECTION_NAME)
    print(f"  ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å: {collection_info.config.params}")
    print(f"  ãƒ™ã‚¯ãƒˆãƒ«æ•°: {collection_info.points_count}")
    print(f"  ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {collection_info.config.params.vectors.size}")

    print("\n" + "=" * 60)
    print("âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
