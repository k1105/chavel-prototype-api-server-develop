#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
wagahai.json ã‹ã‚‰ chunks.jsonl ã‚’å†ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

paragraphå‹ã€conversationå‹ã®textéƒ¨åˆ†ã‚’ç©ã¿é‡ã­ã¦ã€
400æ–‡å­—ã‚’è¶…ãˆã‚‹ã¾ã§ç©ã¿é‡ã­ã€è¶…ãˆãŸå ´åˆã«åˆ¥ã®ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦åˆ†å‰²ã—ã¾ã™ã€‚
"""

import json
import os
import time
import uuid
from pathlib import Path
from typing import Dict, List, Any, Set
from dataclasses import dataclass, asdict
import re

from dotenv import load_dotenv
from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# .env ã‚’ api-server ç›´ä¸‹ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
env_path = Path(__file__).resolve().parents[1] / ".env"
load_dotenv(env_path)

# è¨­å®š
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
COLLECTION_NAME = "neko_scenes"
EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-small")
BATCH_SIZE = 100
TARGET_CHUNK_SIZE = 400  # ç›®æ¨™ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰

# ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
DATA_DIR = Path(__file__).resolve().parents[1] / "data"
WAGAHAI_JSON = DATA_DIR / "wagahai.json"
CHUNKS_JSONL = DATA_DIR / "chunks.jsonl"

# ========= è¨±å¯ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ï¼ˆæ­£è¦åï¼‰ =========
ALLOWED_NAMES: List[str] = [
    "å¾è¼©",
    "ä¸‰æ¯›å­",
    "è»Šå±‹ã®é»’",
    "ç™½",
    "çé‡ è‹¦æ²™å¼¥",
    "è¿·äº­",
    "æ°´å³¶ å¯’æœˆ",
    "è¶Šæ™º æ±é¢¨",
    "å…«æœ¨ ç‹¬ä»™",
    "ç”˜æœ¨å…ˆç”Ÿ",
    "é‡‘ç”°",
    "é‡‘ç”° é¼»å­",
    "é‡‘ç”° å¯Œå­",
    "éˆ´æœ¨ ç±åéƒ",
    "å¤šã€…è‰¯ ä¸‰å¹³",
    "ç‰§å±±",
    "çé‡å¤«äºº",
    "çé‡ ã¨ã‚“å­",
    "çé‡ ã™ã‚“å­",
    "çé‡ ã‚ã‚“å­",
    "å¾¡ä¸‰",
    "é›ªæ±Ÿ",
    "äºŒçµƒç´ã®å¾¡å¸«åŒ ã•ã‚“",
    "å¤äº• æ­¦å³è¡›é–€",
    "å‰ç”° è™è”µ",
    "æ³¥æ£’é™°å£«",
    "å…«ã£ã¡ã‚ƒã‚“",
]

# ========= è¡¨è¨˜ã‚†ã‚Œ â†’ æ­£è¦å =========
ALIASES: Dict[str, str] = {
    "è‹¦æ²™å¼¥": "çé‡ è‹¦æ²™å¼¥",
    "è‹¦æ²™å¼¥å…ˆç”Ÿ": "çé‡ è‹¦æ²™å¼¥",
    "å…ˆç”Ÿï¼ˆè‹¦æ²™å¼¥ï¼‰": "çé‡ è‹¦æ²™å¼¥",
    "å¯’æœˆ": "æ°´å³¶ å¯’æœˆ",
    "è¶Šæ™º": "è¶Šæ™º æ±é¢¨",
    "æ±é¢¨": "è¶Šæ™º æ±é¢¨",
    "ç‹¬ä»™": "å…«æœ¨ ç‹¬ä»™",
    "ç”˜æœ¨": "ç”˜æœ¨å…ˆç”Ÿ",
    "é¼»å­": "é‡‘ç”° é¼»å­",
    "å¯Œå­": "é‡‘ç”° å¯Œå­",
    "ç±åéƒ": "éˆ´æœ¨ ç±åéƒ",
    "è—¤åéƒ": "éˆ´æœ¨ ç±åéƒ",
    "ä¸‰å¹³": "å¤šã€…è‰¯ ä¸‰å¹³",
    "ã¨ã‚“å­": "çé‡ ã¨ã‚“å­",
    "ã™ã‚“å­": "çé‡ ã™ã‚“å­",
    "ã‚ã‚“å­": "çé‡ ã‚ã‚“å­",
    "æ¸…": "å¾¡ä¸‰",
    "ãŠã•ã‚“": "å¾¡ä¸‰",
    "å¾¡å¸«åŒ ã•ã‚“": "äºŒçµƒç´ã®å¾¡å¸«åŒ ã•ã‚“",
    "å¤äº•": "å¤äº• æ­¦å³è¡›é–€",
    "æ­¦å³è¡›é–€": "å¤äº• æ­¦å³è¡›é–€",
    "å‰ç”°": "å‰ç”° è™è”µ",
    "è™è”µ": "å‰ç”° è™è”µ",
    "ã‚„ã£ã¡ã‚ƒã‚“": "å…«ã£ã¡ã‚ƒã‚“",
}

# ========= ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ« =========
@dataclass
class ChunkRec:
    id: str
    chapter: int
    scene_index: int
    start_pos: int
    end_pos: int
    characters: List[str]
    text: str


# ========= ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œå‡º =========
_JP_BOUND = r"(?<![ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9]){name}"

def _contains_name(text: str, name: str) -> bool:
    if name == "ç™½":
        pat = re.compile(rf"(?<![ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9])ç™½(?![ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³A-Za-z0-9])")
        return bool(pat.search(text))
    pat = re.compile(_JP_BOUND.format(name=re.escape(name)))
    return bool(pat.search(text))

def detect_allowed_names(text: str) -> Set[str]:
    found: Set[str] = set()
    # 1) æ­£è¦åã®ç›´æ¥ãƒ’ãƒƒãƒˆ
    for nm in ALLOWED_NAMES:
        if _contains_name(text, nm):
            found.add(nm)
    # 2) ã‚¨ã‚¤ãƒªã‚¢ã‚¹ â†’ æ­£è¦åŒ–
    for alias, canon in ALIASES.items():
        if _contains_name(text, alias):
            found.add(canon)
    return found


# ========= ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ =========
def load_wagahai_json(filepath: Path) -> Dict[str, Any]:
    """wagahai.json ã‚’èª­ã¿è¾¼ã¿"""
    print(f"ğŸ“– {filepath} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    with open(filepath, "r", encoding="utf-8") as f:
        data = json.load(f)
    print(f"âœ“ èª­ã¿è¾¼ã¿å®Œäº†")
    return data

def split_oversize(text: str, max_len: int = TARGET_CHUNK_SIZE) -> List[str]:
    """
    å¤§ãã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã™ã‚‹ã€‚
    å¥ç‚¹ã‚„èª­ç‚¹ãªã©ã®åŒºåˆ‡ã‚Šã§max_lenã‚’è¶…ãˆãªã„æœ€å¾Œã®ä½ç½®ã§åˆ‡ã‚‹ã€‚
    """
    if len(text) <= max_len:
        return [text]
    
    res = []
    i = 0
    n = len(text)
    while i < n:
        end = min(i + max_len, n)
        window = text[i:end]
        if end < n:
            # åŒºåˆ‡ã‚Šæ–‡å­—ã§åˆ†å‰²
            cut = max(
                window.rfind("ã€‚"),
                window.rfind("ã€"),
                window.rfind("ï¼Œ"),
                window.rfind("\n"),
                window.rfind(" "),
            )
            if cut >= 0 and cut >= max_len // 2:
                end = i + cut + 1
        res.append(text[i:end])
        i = end
    return [r.strip() for r in res if r.strip()]

def build_chunks_from_wagahai(data: Dict[str, Any]) -> List[ChunkRec]:
    """wagahai.json ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ"""
    print(f"\nğŸ”¨ ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆä¸­...")
    
    chunks: List[ChunkRec] = []
    scene_idx = 1
    global_pos = 0
    
    chapters = data.get("content", {}).get("chapters", [])
    
    for chapter_data in chapters:
        chapter_id = chapter_data.get("id", 1)
        blocks = chapter_data.get("blocks", [])
        
        current_chunk_text = ""
        current_chunk_start = global_pos
        
        for block in blocks:
            block_type = block.get("type", "")
            block_text = block.get("text", "")
            
            # paragraphå‹ã¨conversationå‹ã®ã¿å‡¦ç†
            if block_type not in ["paragraph", "conversation"]:
                continue
            
            if not block_text.strip():
                continue
            
            # ãƒ–ãƒ­ãƒƒã‚¯ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ†å‰²
            if len(block_text) > TARGET_CHUNK_SIZE:
                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ãŒã‚ã‚Œã°ç¢ºå®š
                if current_chunk_text.strip():
                    scene_id = f"scene_{scene_idx:05d}"
                    current_chunk_end = global_pos
                    
                    characters = sorted(detect_allowed_names(current_chunk_text))[:8]
                    
                    chunks.append(ChunkRec(
                        id=scene_id,
                        chapter=chapter_id,
                        scene_index=scene_idx,
                        start_pos=current_chunk_start,
                        end_pos=current_chunk_end,
                        characters=characters,
                        text=current_chunk_text.strip()
                    ))
                    
                    scene_idx += 1
                    global_pos = current_chunk_end
                    current_chunk_text = ""
                
                # é•·ã„ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆ†å‰²
                block_parts = split_oversize(block_text, TARGET_CHUNK_SIZE)
                for part in block_parts:
                    scene_id = f"scene_{scene_idx:05d}"
                    part_start = global_pos
                    part_end = global_pos + len(part)
                    
                    characters = sorted(detect_allowed_names(part))[:8]
                    
                    chunks.append(ChunkRec(
                        id=scene_id,
                        chapter=chapter_id,
                        scene_index=scene_idx,
                        start_pos=part_start,
                        end_pos=part_end,
                        characters=characters,
                        text=part.strip()
                    ))
                    
                    scene_idx += 1
                    global_pos = part_end
                
                current_chunk_text = ""
                current_chunk_start = global_pos
                continue
            
            # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ ã—ãŸå ´åˆã®æ–‡å­—æ•°ã‚’è¨ˆç®—
            if current_chunk_text:
                test_text = current_chunk_text + block_text
            else:
                test_text = block_text
            
            # 400æ–‡å­—ã‚’è¶…ãˆã‚‹å ´åˆã¯ã€ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç¢ºå®šã—ã¦æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é–‹å§‹
            if len(test_text) > TARGET_CHUNK_SIZE and current_chunk_text:
                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç¢ºå®š
                scene_id = f"scene_{scene_idx:05d}"
                current_chunk_end = global_pos
                
                # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œå‡º
                characters = sorted(detect_allowed_names(current_chunk_text))[:8]
                
                chunks.append(ChunkRec(
                    id=scene_id,
                    chapter=chapter_id,
                    scene_index=scene_idx,
                    start_pos=current_chunk_start,
                    end_pos=current_chunk_end,
                    characters=characters,
                    text=current_chunk_text.strip()
                ))
                
                scene_idx += 1
                global_pos = current_chunk_end
                
                # æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é–‹å§‹
                current_chunk_text = block_text
                current_chunk_start = global_pos
                global_pos += len(block_text)
            else:
                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã«è¿½åŠ 
                if current_chunk_text:
                    current_chunk_text += block_text
                else:
                    current_chunk_text = block_text
                    current_chunk_start = global_pos
                global_pos += len(block_text)
        
        # ç« ã®æœ€å¾Œã«æ®‹ã£ã¦ã„ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’ç¢ºå®š
        if current_chunk_text.strip():
            scene_id = f"scene_{scene_idx:05d}"
            current_chunk_end = global_pos
            
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ¤œå‡º
            characters = sorted(detect_allowed_names(current_chunk_text))[:8]
            
            chunks.append(ChunkRec(
                id=scene_id,
                chapter=chapter_id,
                scene_index=scene_idx,
                start_pos=current_chunk_start,
                end_pos=current_chunk_end,
                characters=characters,
                text=current_chunk_text.strip()
            ))
            
            scene_idx += 1
    
    print(f"âœ“ {len(chunks)} ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ")
    return chunks

def get_overlap_text(text: str, overlap_size: int, from_end: bool = False) -> str:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŒ‡å®šã‚µã‚¤ã‚ºã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—éƒ¨åˆ†ã‚’å–å¾—
    from_end=True: æœ«å°¾ã‹ã‚‰å–å¾—ã€False: å…ˆé ­ã‹ã‚‰å–å¾—
    å¥èª­ç‚¹ã®åŒºåˆ‡ã‚Šã§é©åˆ‡ã«åˆ†å‰²
    """
    if not text or len(text) <= overlap_size:
        return text
    
    if from_end:
        # æœ«å°¾ã‹ã‚‰å–å¾—: æœ€å¾Œã®å¥èª­ç‚¹ã®ä½ç½®ã‚’æ¢ã—ã¦ã€ãã“ã‹ã‚‰50æ–‡å­—ç¨‹åº¦ã‚’å–å¾—
        start_pos = max(0, len(text) - overlap_size - 100)  # å°‘ã—ä½™è£•ã‚’æŒãŸã›ã‚‹
        search_text = text[start_pos:]
        
        # æœ€åˆã®å¥èª­ç‚¹ã®ä½ç½®ã‚’æ¢ã™
        best_pos = 0
        for punct in ["ã€‚", "ã€", "ï¼Œ", "\n"]:
            idx = search_text.find(punct)
            if idx >= 0 and idx < overlap_size:
                best_pos = max(best_pos, idx + 1)
        
        if best_pos > 0:
            overlap_text = search_text[best_pos:]
        else:
            overlap_text = text[-overlap_size:]
        
        # 50æ–‡å­—ã‚’è¶…ãˆãªã„ã‚ˆã†ã«èª¿æ•´
        if len(overlap_text) > overlap_size:
            overlap_text = overlap_text[:overlap_size]
            # æœ€å¾Œã®å¥èª­ç‚¹ã¾ã§æˆ»ã‚‹
            for punct in ["ã€‚", "ã€", "ï¼Œ", "\n"]:
                idx = overlap_text.rfind(punct)
                if idx >= 0:
                    overlap_text = overlap_text[:idx + 1]
                    break
        
        return overlap_text.strip()
    else:
        # å…ˆé ­ã‹ã‚‰å–å¾—: æœ€åˆã®50æ–‡å­—ç¨‹åº¦ã‚’å–å¾—ã—ã€æœ€å¾Œã®å¥èª­ç‚¹ã¾ã§
        overlap_text = text[:overlap_size + 100]  # å°‘ã—ä½™è£•ã‚’æŒãŸã›ã‚‹
        
        # æœ€å¾Œã®å¥èª­ç‚¹ã®ä½ç½®ã‚’æ¢ã™
        best_pos = len(overlap_text)
        for punct in ["ã€‚", "ã€", "ï¼Œ", "\n"]:
            idx = overlap_text.rfind(punct)
            if idx >= 0 and idx >= overlap_size // 2:  # æœ€ä½ã§ã‚‚åŠåˆ†ã¯ç¢ºä¿
                best_pos = min(best_pos, idx + 1)
        
        if best_pos < len(overlap_text):
            overlap_text = overlap_text[:best_pos]
        else:
            overlap_text = text[:overlap_size]
        
        return overlap_text.strip()

def add_overlap_to_chunks(chunks: List[ChunkRec], overlap_size: int = 50) -> List[ChunkRec]:
    """
    ãƒãƒ£ãƒ³ã‚¯é–“ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è¿½åŠ 
    å…ˆé ­ã¨æœ«å°¾ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é™¤ãå„ãƒãƒ£ãƒ³ã‚¯ã«ã€å‰å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
    """
    if len(chunks) <= 1:
        return chunks
    
    print(f"\nğŸ”— ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—è¿½åŠ ä¸­ (ã‚µã‚¤ã‚º: {overlap_size}æ–‡å­—)...")
    
    result_chunks = []
    
    for i, chunk in enumerate(chunks):
        original_text = chunk.text
        new_text = original_text
        
        # å…ˆé ­ã®ãƒãƒ£ãƒ³ã‚¯ã§ãªã„å ´åˆã€å‰ã®ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¾Œ50æ–‡å­—ã‚’è¿½åŠ 
        if i > 0:
            prev_text = chunks[i - 1].text
            prev_overlap = get_overlap_text(prev_text, overlap_size, from_end=True)
            if prev_overlap:
                new_text = prev_overlap + new_text
        
        # æœ«å°¾ã®ãƒãƒ£ãƒ³ã‚¯ã§ãªã„å ´åˆã€æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã®æœ€åˆ50æ–‡å­—ã‚’è¿½åŠ 
        if i < len(chunks) - 1:
            next_text = chunks[i + 1].text
            next_overlap = get_overlap_text(next_text, overlap_size, from_end=False)
            if next_overlap:
                new_text = new_text + next_overlap
        
        # æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿å¤‰æ›´ã€ãã®ä»–ã¯åŒã˜ï¼‰
        new_chunk = ChunkRec(
            id=chunk.id,
            chapter=chunk.chapter,
            scene_index=chunk.scene_index,
            start_pos=chunk.start_pos,
            end_pos=chunk.end_pos,
            characters=chunk.characters,
            text=new_text
        )
        result_chunks.append(new_chunk)
    
    print(f"âœ“ ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—è¿½åŠ å®Œäº†")
    return result_chunks

def write_chunks_jsonl(chunks: List[ChunkRec], filepath: Path):
    """chunks.jsonl ã«æ›¸ãè¾¼ã¿"""
    print(f"\nğŸ’¾ {filepath} ã«æ›¸ãè¾¼ã¿ä¸­...")
    filepath.parent.mkdir(parents=True, exist_ok=True)
    with open(filepath, "w", encoding="utf-8") as f:
        for chunk in chunks:
            f.write(json.dumps(asdict(chunk), ensure_ascii=False) + "\n")
    print(f"âœ“ æ›¸ãè¾¼ã¿å®Œäº†")


# ========= Qdrant ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ =========
def get_embedding_dimension(client: OpenAI, model: str) -> int:
    """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°ã‚’å–å¾—"""
    print(f"\nğŸ“ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« '{model}' ã®æ¬¡å…ƒæ•°ã‚’å–å¾—ä¸­...")
    response = client.embeddings.create(
        model=model,
        input="test"
    )
    dimension = len(response.data[0].embedding)
    print(f"âœ“ æ¬¡å…ƒæ•°: {dimension}")
    return dimension

def create_collection(qdrant: QdrantClient, collection_name: str, dimension: int):
    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆï¼ˆæ—¢å­˜ãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤ã—ã¦å†ä½œæˆï¼‰"""
    print(f"\nğŸ—‚ï¸  ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã‚’æº–å‚™ä¸­...")
    
    # æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ç¢ºèª
    collections = qdrant.get_collections().collections
    exists = any(col.name == collection_name for col in collections)
    
    if exists:
        print(f"âš ï¸  æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã‚’å‰Šé™¤ä¸­...")
        qdrant.delete_collection(collection_name)
        print("âœ“ å‰Šé™¤å®Œäº†")
    
    # æ–°è¦ä½œæˆ
    print(f"âœ“ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã‚’ä½œæˆä¸­...")
    qdrant.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=dimension, distance=Distance.COSINE)
    )
    print("âœ“ ä½œæˆå®Œäº†")

def embed_texts(client: OpenAI, texts: List[str], model: str) -> List[List[float]]:
    """ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
    response = client.embeddings.create(
        model=model,
        input=texts
    )
    return [data.embedding for data in response.data]

def index_chunks_to_qdrant(
    qdrant: QdrantClient,
    openai_client: OpenAI,
    chunks: List[ChunkRec],
    collection_name: str,
    embed_model: str,
    batch_size: int
):
    """ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒãƒƒãƒå‡¦ç†ã§ Qdrant ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
    print(f"\nğŸš€ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆé–‹å§‹ (ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size})")
    
    total = len(chunks)
    indexed_count = 0
    
    for i in range(0, total, batch_size):
        batch = chunks[i:i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (total + batch_size - 1) // batch_size
        
        print(f"ğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches} (ä»¶æ•°: {len(batch)}) ã‚’å‡¦ç†ä¸­...")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿
        texts = [chunk.text for chunk in batch]
        embeddings = embed_texts(openai_client, texts, embed_model)
        
        # ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆ
        points = []
        for chunk, embedding in zip(batch, embeddings):
            point_id = str(uuid.uuid4())
            payload = {
                "scene_id": chunk.id,
                "scene_index": chunk.scene_index,
                "chapter": chunk.chapter,
                "start_pos": chunk.start_pos,
                "end_pos": chunk.end_pos,
                "characters": chunk.characters,
                "text": chunk.text
            }
            points.append(PointStruct(
                id=point_id,
                vector=embedding,
                payload=payload
            ))
        
        # Qdrant ã«ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ
        qdrant.upsert(
            collection_name=collection_name,
            points=points
        )
        
        indexed_count += len(batch)
        print(f"âœ“ {indexed_count}/{total} ä»¶å®Œäº†")
        
        # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚å°‘ã—å¾…æ©Ÿ
        if i + batch_size < total:
            time.sleep(0.5)
    
    print(f"\nâœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†: {indexed_count} ä»¶")

def print_statistics(chunks: List[ChunkRec]):
    """çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›"""
    print("\nğŸ“Š çµ±è¨ˆæƒ…å ±:")
    print(f"  ç·ä»¶æ•°: {len(chunks)}")
    
    if chunks:
        first = chunks[0]
        last = chunks[-1]
        print(f"  æœ€åˆã® scene_index: {first.scene_index} (ç«  {first.chapter})")
        print(f"  æœ€å¾Œã® scene_index: {last.scene_index} (ç«  {last.chapter})")
        
        # ç« ã”ã¨ã®ä»¶æ•°
        chapters = {}
        for chunk in chunks:
            ch = chunk.chapter
            chapters[ch] = chapters.get(ch, 0) + 1
        
        print(f"  ç« ã®æ•°: {len(chapters)}")
        print(f"  ç« ã”ã¨ã®ä»¶æ•°: {dict(sorted(chapters.items()))}")
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®çµ±è¨ˆ
        sizes = [len(chunk.text) for chunk in chunks]
        if sizes:
            print(f"  å¹³å‡ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {sum(sizes) / len(sizes):.1f} æ–‡å­—")
            print(f"  æœ€å°ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {min(sizes)} æ–‡å­—")
            print(f"  æœ€å¤§ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {max(sizes)} æ–‡å­—")


def main():
    print("=" * 60)
    print("ğŸ± å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ - ãƒãƒ£ãƒ³ã‚¯å†ç”Ÿæˆ & Qdrant ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ")
    print("=" * 60)
    
    # wagahai.json ã‚’èª­ã¿è¾¼ã¿
    data = load_wagahai_json(WAGAHAI_JSON)
    
    # ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ
    chunks = build_chunks_from_wagahai(data)
    
    # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è¿½åŠ 
    chunks = add_overlap_to_chunks(chunks, overlap_size=50)
    
    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    print_statistics(chunks)
    
    # chunks.jsonl ã«æ›¸ãè¾¼ã¿
    write_chunks_jsonl(chunks, CHUNKS_JSONL)
    
    # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    # Qdrant ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    print(f"\nğŸ”Œ Qdrant ã«æ¥ç¶šä¸­ ({QDRANT_HOST}:{QDRANT_PORT})...")
    qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
    print("âœ“ æ¥ç¶šæˆåŠŸ")
    
    # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã‚’å–å¾—
    dimension = get_embedding_dimension(openai_client, EMBED_MODEL)
    
    # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
    create_collection(qdrant, COLLECTION_NAME, dimension)
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    index_chunks_to_qdrant(
        qdrant=qdrant,
        openai_client=openai_client,
        chunks=chunks,
        collection_name=COLLECTION_NAME,
        embed_model=EMBED_MODEL,
        batch_size=BATCH_SIZE
    )
    
    # æœ€çµ‚ç¢ºèª
    print("\nğŸ” ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±:")
    collection_info = qdrant.get_collection(COLLECTION_NAME)
    print(f"  ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å: {COLLECTION_NAME}")
    print(f"  ãƒ™ã‚¯ãƒˆãƒ«æ•°: {collection_info.points_count}")
    print(f"  ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {collection_info.config.params.vectors.size}")
    
    print("\n" + "=" * 60)
    print("âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()

